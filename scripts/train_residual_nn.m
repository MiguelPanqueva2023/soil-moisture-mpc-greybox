clc; clear; close all
%% ============================================================
% 1. Load dataset generated by the MPC
% ============================================================
load greybox_dataset.mat   % contains Xdata (Nx4) and Ydata (Nx1)
X = Xdata';   % 4 x N
Y = Ydata';   % 1 x N
N = size(X,2);

%% ============================================================
% 2. Normalization
% ============================================================
xmin = min(X,[],2);
xmax = max(X,[],2);
Xn = (X - xmin) ./ (xmax - xmin + eps);

ymin = min(Y);
ymax = max(Y);
Yn = (Y - ymin) / (ymax - ymin + eps);

%% ============================================================
% 3. NN Architecture (4 → 10 → 6 → 1)
% ============================================================
n_in  = 4;
n_h1  = 10;
n_h2  = 6;
n_out = 1;

rng(1) % Seed for reproducibility
W1 = 0.5*randn(n_h1,n_in);   b1 = zeros(n_h1,1);
W2 = 0.5*randn(n_h2,n_h1);   b2 = zeros(n_h2,1);
W3 = 0.5*randn(n_out,n_h2);  b3 = 0;

%% ============================================================
% 4. Training (Backpropagation from scratch)
% ============================================================
epochs = 3000;
lr = 0.01;
mse = zeros(epochs,1);

for e = 1:epochs
    % Forward pass
    Z1 = W1*Xn + b1;   A1 = tanh(Z1);
    Z2 = W2*A1 + b2;   A2 = tanh(Z2);
    Yhat = W3*A2 + b3;
    
    % Error
    E = Yn - Yhat;
    mse(e) = mean(E.^2);
    
    % Backpropagation
    dY = -2*E/N;
    dW3 = dY*A2';     db3 = sum(dY,2);
    
    dA2 = W3'*dY;     dZ2 = dA2.*(1 - A2.^2);
    dW2 = dZ2*A1';    db2 = sum(dZ2,2);
    
    dA1 = W2'*dZ2;    dZ1 = dA1.*(1 - A1.^2);
    dW1 = dZ1*Xn';    db1 = sum(dZ1,2);
    
    % Weight Update (Gradient Descent)
    W3 = W3 - lr*dW3;  b3 = b3 - lr*db3;
    W2 = W2 - lr*dW2;  b2 = b2 - lr*db2;
    W1 = W1 - lr*dW1;  b1 = b1 - lr*db1;
end

%% ============================================================
% 5. Validation
% ============================================================
Yhat = W3*tanh(W2*tanh(W1*Xn + b1) + b2) + b3;
Ypred = Yhat*(ymax - ymin) + ymin; % Denormalization

% --- Identical Visual Parameters ---
fs = 18;          % Master font size
fs_tit = fs + 4;  
black = [0, 0, 0];

figure('Color','w', 'Units', 'normalized', 'Position', [0.05 0.05 0.9 0.85]);

% Plot with COLORS AND THICKNESS matching the MPC script:
% Real residual in BLUE ('b')
plot(Y, 'b', 'LineWidth', 3); hold on; 

% Prediction in DASHED RED ('r--') like the MPC reference
plot(Ypred, 'r--', 'LineWidth', 2.5); 

% Titles and Labels
title('Residual Neural Network', 'FontSize', fs_tit, 'Color', black, 'FontWeight', 'bold');
xlabel('Sample', 'FontSize', fs, 'Color', black, 'FontWeight', 'bold');
ylabel('Residual', 'FontSize', fs, 'Color', black, 'FontWeight', 'bold');

% Legend
lgd = legend('Real Residual', 'Neural Network', 'Location', 'northeast');
lgd.FontSize = fs-2;
lgd.TextColor = black;
lgd.Color = 'w';
lgd.EdgeColor = black;

% Axes configuration (Identical to MPC script)
grid on;
set(gca, 'Color', 'w', 'XColor', black, 'YColor', black, 'GridColor', black, ...
    'GridAlpha', 0.2, 'FontSize', fs, 'LineWidth', 1.5);

% Limit adjustment for visual clarity
ylim([min(Y)-0.02, max(Y)+0.04]);

%% ============================================================
% 6. Save trained network
% ============================================================
save NN_residual.mat W1 b1 W2 b2 W3 b3 xmin xmax ymin ymax
disp('Trained Neural Network saved: NN_residual.mat')